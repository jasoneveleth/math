<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="jason" />
  <meta name="dcterms.date" content="2021-07-11" />
  <title>Linear Regression</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Linear Regression</h1>
<p class="author">jason</p>
<p class="date">2021-07-11</p>
</header>
<h1 id="theory">Theory</h1>
<p>We are trying to find the line of best fit given a collections of coordinates. For example, <span class="math inline">\(\{(x_1, y_1), (x_2,y_2), (x_3, y_3), \dots\}\)</span>. We would like to find <span class="math inline">\(m,b\)</span> such that <span class="math inline">\(y = mx + b\)</span> minimizes the sum of the residuals squared. By that I mean we want to minimize a function <span class="math inline">\(L(\alpha) = \sum_i (y_i - (mx_i + b))^2\)</span>, by finding the best <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span>.</p>
<p>We need to set up some definitions. Let <span class="math display">\[y = \begin{bmatrix}y_1\\y_2\\y_3\\y_4\\\dots\end{bmatrix},
X = \begin{bmatrix}x_1 &amp; 1\\x_2 &amp; 1\\x_3 &amp; 1\\x_4 &amp; 1\\\dots &amp; \dots\end{bmatrix},
\alpha = \begin{bmatrix}m\\b\end{bmatrix}.\]</span></p>
<p>So our goal is: <span class="math display">\[\min_\alpha L(\alpha)\]</span> <span class="math display">\[=\min_\alpha \sum ((y_i) - (m x_i + b))^2\]</span> Recall from <a href="articles/vectors.html">vectors</a> that <span class="math inline">\(\sum_i x_i^2 = \lVert x \rVert^2\)</span> <span class="math display">\[=\min_\alpha \left\lVert \begin{bmatrix}(y_1 - (m x_1 + b))\\(y_2 - (m x_2 + b))\\(y_3 - (m x_3 + b))\\(y_4 - (m x_4 + b))\\\dots\end{bmatrix}\right\rVert^2\]</span> <span class="math display">\[=\min_\alpha \lVert y - X\alpha\rVert^2\]</span> Recall that <span class="math inline">\(\lVert x \rVert^2 = x^Tx\)</span>. <span class="math display">\[=\min_\alpha (y-X\alpha)^T(y - X\alpha)\]</span> Recall that <span class="math inline">\((A + B)^T = A^T + B^T\)</span> and <span class="math inline">\((AB)=B^TA^T\)</span>. <span class="math display">\[=\min_\alpha (y^T - \alpha^TX^T)(y - X\alpha)\]</span> <span class="math display">\[=\min_\alpha y^Ty - \alpha^TX^Ty - y^TX\alpha + \alpha^TX^TX\alpha\]</span></p>
<p>To minimize this, we find the critical points of the function. We do this by finding where the <a href="articles/gradient.html">gradient</a> of <span class="math inline">\(L(\alpha)\)</span> is <span class="math inline">\(0\)</span>.</p>
<p><span class="math display">\[0 = \frac{d}{d\alpha} L(\alpha)\]</span> <span class="math display">\[= \frac{d}{d\alpha} (y^Ty - \alpha^TX^Ty - y^TX\alpha + \alpha^TX^TX\alpha)\]</span> <span class="math display">\[=\frac{d}{d\alpha} (y^Ty) - \frac{d}{d\alpha} (\alpha^TX^Ty) - \frac{d}{d\alpha} (y^TX\alpha) + \frac{d}{d\alpha} (\alpha^TX^TX\alpha)\]</span> Recall from <a href="articles/vector_derivatives.html">vector derivatives</a>: <span class="math inline">\(\frac{d}{dx} ( u^Tx) = u^T\)</span>, <span class="math inline">\(\frac{d}{dx} (x^Tu) = u^T\)</span>, and <span class="math inline">\(\frac{d}{dx}(x^Tx) = \frac{d}{dx}(\bar{x}^Tx) + \frac{d}{dx}(x^T\bar{x})\)</span>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span class="math display">\[= 0 - (X^Ty)^T - y^TX + \frac{d}{d\alpha} (\alpha^TX^TX\bar{\alpha}) + \frac{d}{d\alpha} (\bar{\alpha}^TX^TX\alpha)\]</span> <span class="math display">\[= -y^TX - y^TX + (X^TX\alpha)^T + \alpha^TX^TX\]</span> <span class="math display">\[=-2y^TX + \alpha^T(X^TX)^T + \alpha^TX^TX\]</span> <span class="math display">\[=-2y^TX + \alpha^TX^TX + \alpha^TX^TX\]</span> <span class="math display">\[=-2y^TX + 2\alpha^TX^TX\]</span></p>
<p>And now for the glorious part (who am I kidding, this whole thing has been glorious),</p>
<p><span class="math display">\[2y^TX=2\alpha^TX^TX\]</span> <span class="math display">\[y^TX=\alpha^T(X^TX)\]</span> <span class="math display">\[y^TX(X^TX)^{-1} = \alpha^T = \begin{bmatrix}m &amp; b\end{bmatrix}\]</span></p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>The bar in <span class="math inline">\(\bar{\alpha}\)</span> or <span class="math inline">\(\bar{x}\)</span> means treat it as a constant, in the same way that you would if you were doing the product rule with single valued functions: <span class="math inline">\(\frac{d}{dx}(f(x)g(x)) = \frac{d}{dx}(f(x)\bar{g}(x)) +  \frac{d}{dx}(\bar{f}(x) g(x))\)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
